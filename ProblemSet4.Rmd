---
title: "The 3-billion School Improvement Grants (SIG) program has no significant effects on student outcome "
author: "Sharon Allman, Diego Mamanche Castellano & Ke-Li Chiu"
date: "11/03/2020"
output: pdf_document
abstract: "The objective of this analysis was to examine if there was a relationship between a school’s participation in a School Improvement Grant program and an increase in that school’s students’ achievement.  Using linear regression models, we failed to reject the null hypothesis that the SIG programs had no statistically significant effect on students’ performance over time."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE)## R Markdown
```

```{r echo=FALSE, include=FALSE, message=FALSE}
#Set up the environment
library(readxl)
library(dplyr)
library(tidyr)
library(ggplot2)
library(reshape2)
library(stringr)
library(knitr)
library(tidyverse)
library(broom)
library(huxtable)
library(jtools)
#setwd("~/Experimental Design for Data Science/ProblemSet4")
```

```{r echo=FALSE, include=FALSE, message=FALSE}
###Upload datasets
treated_schools <- read_excel("sy1011-1314.xlsx")
treated_schools <- janitor::clean_names(treated_schools)
math_assessment_09_10 <- read.csv("math-achievement-sch-sy2009-10.csv")
math_assessment_09_10 <- janitor::clean_names(math_assessment_09_10)
math_assessment_13_14 <- read.csv("math-achievement-sch-sy2013-14.csv")
math_assessment_13_14 <- janitor::clean_names(math_assessment_13_14)
head(treated_schools)
```

```{r echo=FALSE, include=FALSE, message=FALSE}
treated_all_cohorts <- filter(treated_schools, 
                              sy201011sig_model != is.na(sy201011sig_model) &
                              sy201112sig_model != is.na(sy201112sig_model) &
                              sy201213sig_model != is.na(sy201213sig_model) &
                              sy201314sig_model != is.na(sy201314sig_model) &
                              ncessch_1011 == ncessch_1112 &
                              ncessch_1011 == ncessch_1213 &
                              ncessch_1011 == ncessch_1314 &
                              ncessch_1112 == ncessch_1213 &
                              ncessch_1112 == ncessch_1314 &
                              ncessch_1213 == ncessch_1314 
                              )
treated_all_cohorts <- select(treated_all_cohorts, state, leaid_10_11, 
                           leanm_1011, ncessch_1011, schnam_1011,
                           sy201011sig_model, sy201112sig_model,
                           sy201213sig_model, sy201314sig_model)
treated_all_cohorts$ncessch_1011 <- as.numeric(treated_all_cohorts$ncessch_1011)
colnames(treated_all_cohorts) <- c("state", "lea_id","lea_name", 
                                   "ncessch","school_name","model_2010_11", "model_2011_12","model_2012_13","model_2013_14")
treated_all_cohorts
```
```{r echo=FALSE, include=FALSE, message=FALSE}
treated_math_09_10 <- select(math_assessment_09_10, stnam, leaid, 
                             leanm, ncessch, schnam09, all_mth00pctprof_0910)
treated_math_13_14 <- select(math_assessment_13_14, stnam, leaid, 
                              leanm, ncessch, schnam, all_mth00pctprof_1314)
merged_math <- merge(treated_math_09_10, treated_math_13_14, by= "ncessch")
merged_math <- select(merged_math, ncessch, schnam09, stnam.x, 
                      leaid.x, leanm.x, all_mth00pctprof_0910, all_mth00pctprof_1314)
merged_math
```
```{r echo=FALSE, include=FALSE, message=FALSE}
### Create Treatment DataSet ###
results_treated <- merge(treated_all_cohorts, merged_math, by= "ncessch")
results_treated <- filter(results_treated,
                          !grepl("(([0-9]+[-][0-9]+)|([a-zA-Z]+[0-9]+)|([a-zA-z]+))",
                                 all_mth00pctprof_0910) )
results_treated <- filter(results_treated, 
                          !grepl("(([0-9]+[-][0-9]+)|([a-zA-Z]+[0-9]+)|([a-zA-z]+))",
                                 all_mth00pctprof_1314) )
results_treated <- select(results_treated, ncessch, schnam09, stnam.x, 
                          leaid.x, leanm.x, all_mth00pctprof_0910, all_mth00pctprof_1314)
colnames(results_treated) <- c("ncessch", "school_name", "state", "lea_id",
                               "lea_name", "scores_0910", "scores_1314")
results_treated
```
# Introduction

In 2009, the US Government allocated $3 billion dollars under the American Recovery and Reinvestment Act of 2009 into School Improvement Grants (SIG) to turn around the lowest performing schools. As one of the Obama administration’s signature programs, SIG is also one of the largest investments in education grants made by the US government. Did SIG drive changes in outcome of schools’ performance and students’ achievement? That is the 3-billion-dollar question.   

The SIG awarded persistently low-performing schools sizeable funding to bring about radical school changes by implementing their intervention models that included specific practices such as increased learning time, integration of technology in classrooms, and the replacement of administration management. In this study, we evaluated the impact of the SIG program by observing the schools that have implemented the models in all four years to see the change of percentage of students meeting a level of proficiency in math assessments. As a result, despite the substantial amount of investment, we found no statistically significant effect of the program on the students’ proficiency outcome.   

# Dataset Description and Data Cleaning 

The dataset sy1011-1314 includes data for four school years from 2010-11 to 2013-14. The data contains the list of schools who got the funding to implement SIG programs in each year. Year 2010-11 is implementation year 1, 2011-12 is implementation year 2, 2012-13 is implementation year 3, and 2013-14 is implementation year 4. We kept only the rows of schools who had implemented SIG models in all four implementation years. It is possible to assess the program in each implementation year. However, we choose to observe the long-term effect of the program. Therefore, the data from 2009-10 (before implementation year 1) is compared with data from implementation year 4.  

To assess the effect of the SIG program, we wanted to observe the percentage change of students in the schools that scored above proficiency in math assessment before implementation 1 (2009-10) and at implementation year 4 (2013-14). Therefore, we gathered math achievement data sets from school years 2009-10 and 2013-14 and extracted the column of math proficiency rate from each data set. The math proficiency rate has a range from 0 to 100. Moreover, the datasets contain schools that did not implement SIG programs in any of the implementation years, which allowed us to formulate a control group with random assignment method. In the end, we had a data frame observing 183 schools across 5 school years with 8 variables concerning the schools’ name, region, model implemented, math proficiency rate, and school year.      

# Method 

The method we used to estimate the effects of the SIG programs was difference-in-differences (diff-in-diff). The initial difference between the treatment and control group was taken into account. Because the SIG program only awarded the schools that ranked as low performing, the average proficiency rate in the treatment group was substantially lower than the ones in the control group— hence we did not compare the differences of proficiency rate between treatment and control group. What we wanted to compare were the changes in the treatment group over time to changes in the control group over time. 

# Summary Statistics

The distribution of percentage rate in the treated group in the school years 2009-10 and 2013-14 are shown in Figure 1. We can see that most of the schools in the treated group have a proficiency rate under 50%, meaning a large portion of the students among these schools did not meet the proficient level in math assessment.    

```{r echo=FALSE, message=FALSE, fig1, fig.height = 2, fig.width = 7}
# Historgram of percentages of all cohorts
library(tidyr)
library(ggplot2)
plot <- select(results_treated, "scores_0910","scores_1314")
plot <- as.data.frame(plot)
plot[,1:2] <- sapply(plot[,1:2], function(x) as.character(x))
plot[,1:2] <- sapply(plot[,1:2], function(x) as.numeric(x))
plot <- as.data.frame(plot)

plot <- plot %>%
  gather(key="text", value="value") %>%
  mutate(text = gsub("\\.", " ",text)) %>%
  mutate(value = round(as.numeric(value),0))

plot %>%
  ggplot( aes(x=value, color=text, fill=text)) +
    geom_histogram(alpha=0.6, binwidth = 5) +
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
    xlab("Percentage") +
    ylab("Counts") +
    facet_wrap(~text)
```
Figure 1. Distribution of math proficiency rate in the treated group in 2009-10 and 2013-14  

```{r echo=FALSE, include=FALSE, message=FALSE}
diff_in_diff_treated <- melt(results_treated, measure = 6:7)
diff_in_diff_treated[,7] <- 
sapply(diff_in_diff_treated[,7], function(x) as.numeric(x))
colnames(diff_in_diff_treated) <- c("ncessch", "school_name", "state", "lea_id",
                               "lea_name", "cohort","percentage")
diff_in_diff_treated <- filter(diff_in_diff_treated, cohort == 
                                 "scores_0910" | cohort == "scores_1314")
```
```{r echo=FALSE, include=FALSE, message=FALSE}
### Create Control Group ###
merged_math_cleaned <- merged_math
unique_treated_schools <- melt(treated_schools, measure = 10:13)
unique_treated_schools <- unique(unique_treated_schools$value)
merged_math_cleaned <- filter(merged_math, !(ncessch %in% 
                                as.numeric(unique_treated_schools)))
merged_math_cleaned <- filter(merged_math_cleaned,
                          !grepl("(([0-9]+[-][0-9]+)|([a-zA-Z]+[0-9]+)|([a-zA-z]+))",
                                 all_mth00pctprof_0910) )
merged_math_cleaned <- filter(merged_math_cleaned, 
                          !grepl("(([0-9]+[-][0-9]+)|([a-zA-Z]+[0-9]+)|([a-zA-z]+))",
                                 all_mth00pctprof_1314) )
results_control <- data.frame()
for (i in unique(results_treated$state)) {
    count_state <- count(filter(results_treated, state == i))
    filter_by_state <- which(merged_math_cleaned$stnam.x == i)
    set.seed(123)
    sample_by_state <- merged_math_cleaned[sample(filter_by_state,
                                                  as.numeric(count_state)),]
    results_control <- rbind(results_control, sample_by_state)
    }
colnames(results_control) <- c("ncessch", "school_name", "state", "lea_id",
                               "lea_name", "scores_0910", "scores_1314")
diff_in_diff_controled <- melt(results_control, measure = 6:7)
diff_in_diff_controled[,7] <- 
sapply(diff_in_diff_controled[,7], function(x) as.numeric(x))
colnames(diff_in_diff_controled) <- c("ncessch", "school_name", "state", "lea_id",
                               "lea_name", "cohort","percentage")
```
```{r echo=FALSE, include=FALSE, message=FALSE}
### Create diff in diff dataset with both treated and controlled groups ###
diff_in_diff_treated <- mutate(diff_in_diff_treated, sig_program = "1")
diff_in_diff_controled <- mutate(diff_in_diff_controled, sig_program = "0")
diff_in_diff_both <- rbind(diff_in_diff_controled, diff_in_diff_treated)
```

The distribution of the treatment group before and after the program implementation is shown in Figure 2. The mean of the proficiency rate in both school years remained under 40%. The maximum proficiency rate was 93% in the school year 2009-2010. The lowest proficiency rate was 2% in school year 2013-2014.   

```{r echo=FALSE, include=TRUE, message=FALSE, fig2, fig.height = 2, fig.width = 7}
boxplot(percentage ~ cohort, data = diff_in_diff_treated,
        xlab = "School Year", las = 2, 
        ylab = "Proficiency Rate", main = "Treatment Group Boxplot")
```
Figure 2. Treatment group summary statistics 

The distribution of the control group is shown in Figure 3. In the control group, the mean of the proficiency rate across the five school years was above 60%. The maximum proficiency rate was 98% in both school years 2009-2010 and 2013-2014. The lowest proficiency rate was 5% in the school year 2013-2014.  

```{r echo=FALSE, include=TRUE, message=FALSE, fig3, fig.height = 2, fig.width = 7}
boxplot(percentage ~ cohort, data = diff_in_diff_controled,
        xlab = "School Year", las = 2, 
        ylab = "Proficiency Rate", main = "Control Group Boxplot")
```
Figure 3. Control group summary statistics  

# Research Question and Hypotheses

Within this research, we aimed to examine the impact of SIG programs over time.  

H0: The SIG programs had no effect on students’ performance over time. 

H1: The SIG programs had an effect on students’ performance over time. 

H2: The SIG programs had a positive effect on students’ performance over time. 

# Ethical Issues

This data was sourced from the United States Department of Education and is publicly available, so this data has been largely anonymized. This data does not contain specific demographic information that could directly identify any of the participants.  However, it does contain very specific information about the schools involved, especially the names of the schools and the states they are located in.  This may be problematic as this could be theoretically used to de-anonymize data specific to individuals if combined with other demographic data (about students/participants in relation to these states or the names of these schools).  It is unlikely that individual respondents would be able to request to have data related to them removed from this data, as this is data collected by the government and does not have personally identifying information.  Additionally, clearly identifying the worst-performing schools could be further stigmatizing to the institutions and individuals associated with them.   

This data is representative of the program as a whole, as this data is cumulative of all of the cases of this program within the specific time frames. Research done using this data may have higher external validity, as this data is representative of the whole population impacted by the program. However, it may be ethically ambiguous for the government to be publishing positive findings about their program (The U.S. Department of Education, 2013), when independent research sources (including us) were unable to demonstrate that this program had a significant impact (Le Floch, 2018).   

The United States Department of Education has broad ethical guidelines in terms of collection, dissemination, and analysis of data.  However, ethical considerations specific to this program are not outlined in the data documentation or summary of findings.  A multidisciplinary team was assembled in order to conceptualize and implement these programs (Kutash et al., 2010), however, it is unclear if this was also the case when it came to the data collection and analysis.  This means that there may be some selection bias when it comes to the types of variables measured and reported on by the researchers.  This could have been mitigated by ensuring they reported a transparent process of how the data was collected and analyzed, including the documentation and human resources used to reduce sampling bias.   

# Weaknesses & Limitations

As previously mentioned, research done using this data could theoretically have higher external validity, as this data is directly representative of the population impacted by the program. As this was a government-run program, analyzed by the government itself, the sampling included the entire population, ensuring that the sample would be fully representative. Another consideration is that this data spans from 2009 to 2014.  While it may be representative of that specific timeframe, it would not be representative of school settings today.  Additionally, those organizing and analyzing this data have done so in a particular way, which may have included some sampling bias as they would have selected the measured variables according to what they had already planned to study.  

# Analysis

## Construction of Treatment Group and Control Group 

To assure the comparability of the treatment group and the control group, we ensured that the two groups had the composition in the distribution of schools across states. The distribution of schools across states in treatment group is shown in Figure 4, and the control group distribution is shown in Figure 5. 

```{r echo=FALSE, include=TRUE, message=FALSE, fig4, fig.height = 2, fig.width = 7}
ggplot(results_treated, aes(x=state)) +
  geom_bar()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```
Figure 4. Distribution of schools which implemented the SIG models across states 

```{r echo=FALSE, include=TRUE, message=FALSE, fig5, fig.height = 2, fig.width = 7}
ggplot(results_control, aes(x=state)) +
  geom_bar()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```
Figure 5. Distribution of schools which did not implement the SIG models across states  

We plotted the proficiency rate change between 2009-10 and 2013-14 of each school in both treatment and control groups. As shown in Figure 6, there is no clear trend in either group.  

```{r echo=FALSE, include=TRUE, message=FALSE, fig6, fig.height = 3, fig.width = 7}
diff_in_diff_both %>% 
  ggplot(aes(x = cohort,
             y = percentage,
             color = sig_program
             )) +
  geom_point() +
  geom_line(aes(group = ncessch), alpha = 0.2) +
  theme_minimal() +
  labs(x = "Cohorts",
       y = "Percentage",
       color = "SIG pogram") +
  scale_color_brewer(palette = "Set1")
```
Figure 6. Proficiency rate of each school in both treatment and control groups 

## Linear Regression  

Linear regression is applied to evaluate the effect of the SIG programs on proficiency rate across four school years. The equation used in the models is:  

$$ Y_{i,t} = \beta0 + \beta1 \text{(Treatment group)}_{i} + \beta2 \text{(Year)}_{t} +  
\beta3 \text{(Treatment group} × \text{Year})_{i,t} + \epsilon_{i,t}$$

$\beta3$ is how much the treatment group changes after the treatment event compared to how much the control group changes after the treatment event, which is the term we care about.  

### Model 1 
Model 1 observes the change between school year 2009-10 and 2013-2014. As the results shown in Table 1 outline, the average difference of the differences is 4.20. The treatment group in year 2013-2014 has a 4.2 percent increase in math assessment rate on average. However, the p-value of $\beta3$ is 0.06—the effect of SIG program on math proficiency rate is insignificant.  

```{r echo=FALSE, include=TRUE, message=FALSE}
cohort1 <- filter(diff_in_diff_both, cohort %in% c("scores_0910","scores_1314"))
cohort1$cohort <- if_else(cohort1$cohort == "scores_0910", "0","1")
cohort1$sig_program <- as.factor(cohort1$sig_program)
cohort1$cohort <- as.factor(cohort1$cohort)
cohort1 <- 
  rename(cohort1, 
    year = cohort
    )
diff_in_diff_cohort1_model <- lm(percentage ~  sig_program*year, 
                         data = cohort1)
summ(diff_in_diff_cohort1_model, model.info = FALSE, model.fit = FALSE)
```
Table 1. OLS for diff-in-diff between 2009-10 and 2013-2014  

\pagebreak
# Appendix A

```{r ref.label=knitr::all_labels(), echo = T, eval = F, tidy=TRUE, tidy.opts=list(width.cutoff=60), results='asis'}
```

\pagebreak
# References

Dragoset, L., Thomas, J., Herrmann, M., Deke, J., &amp; James-Burdumy, S. (2017, January 27). School Improvement Grants: Implementation and Effectiveness Executive Summary (Rep.). Retrieved March 19, 2020, from U.S. Department of Education website: https://ies.ed.gov/ncee/pubs/20174013/pdf/20174012.pdf

Hadley Wickham and Lionel Henry (2019). tidyr: Tidy Messy Data. R package version 1.0.0.
https://CRAN.R-project.org/package=tidyr

Kutash, J., Nico, E., Gorin, E., Rahmatullah, S., & Tallant, K. (2010). The school turnaround field guide. FSG Social Impact Advisors. https://www.wallacefoundation.org/knowledge-center/Documents/The-School-Turnaround-Field-Guide.pdf 

Le Floch, K. C. (2018, January 22). Did school improvement grants work anywhere? American Institutes for Research. https://www.air.org/resource/did-school-improvement-grants-work-anywhere 

Public Impact, EdPlex, & Michael & Susan Dell Foundation. (2020). What is restart and why do it? School Restarts. https://www.schoolrestarts.org/what-is-restart-and-why-do-it/ 

R Core Team (2019). R: A language and environment for statistical computing.  

R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/. 

The U.S. Department of Education. (2013). School improvement grants national summary school year 2012-13. https://www2.ed.gov/programs/sif/data/school-impro-grants-nat-sum-sy1213.pdf 

The U.S. Department of Education. (2015). SIG transformation model – required and permissible activities. https://www.alsde.edu/ofc/osl/SIG/SIG%20Transformation%20Model.pdf 

Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software,
4(43), 1686, https://doi.org/10.21105/joss.01686

Yihui Xie (2019). knitr: A General-Purpose Package for Dynamic Report Generation in R. R
package version 1.25.

Yihui Xie (2015) Dynamic Documents with R and knitr. 2nd edition. Chapman and Hall/CRC.
ISBN 978-1498716963
